{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 · Modelado de clustering (activos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# PASO 0 — CHECKLIST COHERENCIA (NB-03)\n",
        "# =========================\n",
        "# Objetivo: antes de auditar a fondo (Paso 1), verificar que los datasets de entrada\n",
        "# cumplen lo acordado en NB-01 y NB-02 y están alineados para el clustering.\n",
        "\n",
        "# ────────────────────────\n",
        "# 1) Imports y configuración base de proyecto\n",
        "# ────────────────────────\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Asegura que podamos importar módulos del repo si hiciera falta (src/, etc.)\n",
        "ROOT = Path.cwd().parent  # carpeta raíz del proyecto (sube un nivel desde /notebooks)\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))\n",
        "\n",
        "# Carga variables de entorno (permite override de rutas sin tocar el código)\n",
        "load_dotenv()\n",
        "\n",
        "# Opcional: que no se corten columnas en displays\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", None)\n",
        "\n",
        "# ────────────────────────\n",
        "# 2) Resolución de rutas (con .env y defaults)\n",
        "# ────────────────────────\n",
        "# Patrón de nombres de variables de entorno (puedes cambiarlos si ya definiste otros):\n",
        "# - X_BASE_PATH:              ruta a activos_for_model_v2.csv (baseline, sin Empresa)\n",
        "# - X_EMPRESA_PATH:           ruta a activos_for_model_v2_empresa.csv (con Empresa)\n",
        "# - X_IDS_PATH:               ruta a activos_ids_v2_plus.csv (IDs de reenganche)\n",
        "#\n",
        "# Defaults (si .env no define nada): en data/processed/ bajo ROOT.\n",
        "\n",
        "DEFAULT_BASE = ROOT / \"data\" / \"processed\" / \"activos_for_model_v2.csv\"\n",
        "DEFAULT_EMP = ROOT / \"data\" / \"processed\" / \"activos_for_model_v2_empresa.csv\"\n",
        "DEFAULT_IDS = ROOT / \"data\" / \"processed\" / \"activos_ids_v2_plus.csv\"\n",
        "\n",
        "X_BASE_PATH = Path(os.getenv(\"X_BASE_PATH\", str(DEFAULT_BASE)))\n",
        "X_EMP_PATH  = Path(os.getenv(\"X_EMPRESA_PATH\", str(DEFAULT_EMP)))\n",
        "X_IDS_PATH  = Path(os.getenv(\"X_IDS_PATH\", str(DEFAULT_IDS)))\n",
        "\n",
        "# ────────────────────────\n",
        "# 3) Carga de datos con mensajes claros de error\n",
        "# ────────────────────────\n",
        "def _leer_csv_seguro(path: Path, nombre_log: str) -> pd.DataFrame:\n",
        "    \"\"\"Lee un CSV y lanza un error amigable si no existe o falla la lectura.\"\"\"\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"[{nombre_log}] No se encontró el archivo en: {path}\\n\"\n",
        "            f\"→ Revisa tu .env (X_*_PATH) o coloca el archivo en la ruta por defecto.\"\n",
        "        )\n",
        "    try:\n",
        "        return pd.read_csv(path, low_memory=False)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            f\"[{nombre_log}] Error al leer el CSV en {path}:\\n{e}\"\n",
        "        )\n",
        "\n",
        "df_base     = _leer_csv_seguro(X_BASE_PATH, \"BASE (sin Empresa)\")\n",
        "df_empresa  = _leer_csv_seguro(X_EMP_PATH,  \"CON EMPRESA\")\n",
        "df_ids      = _leer_csv_seguro(X_IDS_PATH,  \"IDS\")\n",
        "\n",
        "print(\"RUTAS USADAS\")\n",
        "print(\"  Base (sin Empresa):\", X_BASE_PATH)\n",
        "print(\"  Con Empresa       :\", X_EMP_PATH)\n",
        "print(\"  IDs               :\", X_IDS_PATH)\n",
        "print()\n",
        "\n",
        "# ────────────────────────\n",
        "# 4) Checks principales (shapes y consistencia de filas)\n",
        "# ────────────────────────\n",
        "print(\"SHAPES\")\n",
        "print(\"  Base (sin Empresa):\", df_base.shape)\n",
        "print(\"  Con Empresa       :\", df_empresa.shape)\n",
        "print(\"  IDs               :\", df_ids.shape)\n",
        "print()\n",
        "\n",
        "same_rows_be = (df_base.shape[0] == df_empresa.shape[0])\n",
        "same_rows_bi = (df_base.shape[0] == df_ids.shape[0])\n",
        "print(\"¿Mismo nº de filas Base vs Empresa?  \", same_rows_be)\n",
        "print(\"¿Mismo nº de filas Base vs IDs?      \", same_rows_bi)\n",
        "if not (same_rows_be and same_rows_bi):\n",
        "    print(\"⚠️ Advertencia: los datasets no tienen el mismo nº de filas. Revisa el pipeline de NB-02.\")\n",
        "print()\n",
        "\n",
        "# ────────────────────────\n",
        "# 5) Columnas de presencias (deben ser EXACTAMENTE 4: 15d, 1m, 3m, 6m)\n",
        "# ────────────────────────\n",
        "pres_cols = [c for c in df_base.columns if c.endswith(\"_pres\")]\n",
        "print(\"Columnas de presencias detectadas en Base:\", pres_cols)\n",
        "\n",
        "esperadas_pres = {\"Atencion15d_pres\", \"Atencion1m_pres\", \"Atencion3m_pres\", \"Atencion6m_pres\"}\n",
        "faltan_pres = esperadas_pres - set(pres_cols)\n",
        "sobran_pres = set(pres_cols) - esperadas_pres\n",
        "\n",
        "print(\"¿FALTAN presencias esperadas? \", faltan_pres if faltan_pres else \"No\")\n",
        "print(\"¿SOBRAN presencias no esperadas?\", sobran_pres if sobran_pres else \"No\")\n",
        "print()\n",
        "\n",
        "# ────────────────────────\n",
        "# 6) Numéricas clave (KPIs y cantidades + edad)\n",
        "# ────────────────────────\n",
        "numericas_clave = [\n",
        "    \"Edad\", \"CantPptos\", \"CantPptosAbo\", \"CantPptosAvan\",\n",
        "    \"TotPptos_l1p\", \"TotPptosAbo_l1p\", \"TotPptosAvan_l1p\",\n",
        "    \"PctCumplimiento\", \"TicketPromPpto_l1p\"\n",
        "]\n",
        "faltantes_num = [c for c in numericas_clave if c not in df_base.columns]\n",
        "print(\"Numéricas clave faltantes en Base:\", faltantes_num if faltantes_num else \"Ninguna\")\n",
        "print()\n",
        "\n",
        "# ────────────────────────\n",
        "# 7) Columnas adicionales en el set CON EMPRESA (deben incluir Empresa/Convenio)\n",
        "# ────────────────────────\n",
        "cols_extra = list(set(df_empresa.columns) - set(df_base.columns))\n",
        "# Orden alfabético solo para leer más fácil:\n",
        "cols_extra = sorted(cols_extra)\n",
        "ejemplos_esperados = (\"TieneEmpresa\", \"EsConvenio\", \"Empresa_grp_\")\n",
        "print(\"Columnas adicionales (Con Empresa vs Base):\", cols_extra[:15], (\"… (+)\" if len(cols_extra) > 15 else \"\"))\n",
        "print(\"¿Incluye señales esperadas?\",\n",
        "      any(c == \"TieneEmpresa\" for c in cols_extra) and\n",
        "      any(c == \"EsConvenio\" for c in cols_extra) and\n",
        "      any(c.startswith(\"Empresa_grp_\") for c in cols_extra))\n",
        "print()\n",
        "\n",
        "# ────────────────────────\n",
        "# 8) NaNs totales (ideal: 0, según NB-02 imputación ya aplicada)\n",
        "# ────────────────────────\n",
        "nans_base = int(df_base.isna().sum().sum())\n",
        "nans_emp  = int(df_empresa.isna().sum().sum())\n",
        "print(\"NaNs totales en Base:   \", nans_base)\n",
        "print(\"NaNs totales en Empresa:\", nans_emp)\n",
        "if nans_base > 0 or nans_emp > 0:\n",
        "    print(\"⚠️ Advertencia: hay NaNs. Revisa imputación de NB-02 antes de escalar y clusterizar.\")\n",
        "print()\n",
        "\n",
        "# ────────────────────────\n",
        "# 9) (Opcional) Vista rápida de 5 filas para olfateo\n",
        "# ────────────────────────\n",
        "print(\"Muestra Base (5 filas):\")\n",
        "display(df_base.head())\n",
        "\n",
        "print(\"Muestra Con Empresa (5 filas):\")\n",
        "display(df_empresa.head())\n",
        "\n",
        "print(\"Muestra IDs (5 filas):\")\n",
        "display(df_ids.head())\n",
        "\n",
        "# ────────────────────────\n",
        "# 10) Mini-tests (asserts suaves con mensajes claros)\n",
        "# ────────────────────────\n",
        "def _ok(flag: bool) -> str:\n",
        "    return \"OK ✅\" if flag else \"FALLO ❌\"\n",
        "\n",
        "tests = {\n",
        "    \"FILAS_base_vs_empresa\": same_rows_be,\n",
        "    \"FILAS_base_vs_ids\"    : same_rows_bi,\n",
        "    \"PRES_existen_4\"       : (set(pres_cols) == esperadas_pres),\n",
        "    \"NUM_no_faltantes\"     : (len(faltantes_num) == 0),\n",
        "    \"EMP_columnas_extra\"   : (\n",
        "        any(c == \"TieneEmpresa\" for c in cols_extra) and\n",
        "        any(c == \"EsConvenio\" for c in cols_extra) and\n",
        "        any(c.startswith(\"Empresa_grp_\") for c in cols_extra)\n",
        "    ),\n",
        "    \"NANS_cero_base\"       : (nans_base == 0),\n",
        "    \"NANS_cero_empresa\"    : (nans_emp == 0),\n",
        "}\n",
        "\n",
        "print(\"\\nRESULTADO MINI-TESTS\")\n",
        "for k, v in tests.items():\n",
        "    print(f\"  {k:22s}: {_ok(v)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ebf1b93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PASO 1 — AUDITORÍA DE DATOS (NB-03) — SOLO INSPECCIÓN\n",
        "# ============================================================\n",
        "# Objetivo: ver el \"esquema real\" de los datasets que usaremos en el clustering,\n",
        "# SIN transformar nada. De este paso vamos a salir con listas de columnas\n",
        "# (numéricas / presencias / dummies) listas para el pipeline del Paso 2.\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 1) Vista general (shape, columnas, tipos, nulos) — ambos sets\n",
        "# ────────────────────────────────────────────────────────────\n",
        "print(\"=== RUTAS USADAS ===\")\n",
        "print(\"Base (sin Empresa):\", X_BASE_PATH)                                             # confirma ruta efectiva\n",
        "print(\"Con Empresa       :\", X_EMP_PATH)\n",
        "print(\"IDs               :\", X_IDS_PATH)\n",
        "print()\n",
        "\n",
        "print(\"=== SHAPES ===\")\n",
        "print(\"Base    :\", df_base.shape)                                                     # filas x columnas baseline\n",
        "print(\"Empresa :\", df_empresa.shape)                                                  # filas x columnas con Empresa\n",
        "print(\"IDs     :\", df_ids.shape)                                                      # filas x columnas IDs\n",
        "print()\n",
        "\n",
        "print(\"=== TIPOS (dtypes) — Base (TOP 10) ===\")\n",
        "print(df_base.dtypes.sort_index().head(10))                                           # muestra 10 tipos para ver variedad\n",
        "print(\"… (total columnas Base:\", df_base.shape[1], \")\")\n",
        "print()\n",
        "\n",
        "print(\"=== TIPOS (dtypes) — Con Empresa (TOP 10) ===\")\n",
        "print(df_empresa.dtypes.sort_index().head(10))                                        # idem para variante con Empresa\n",
        "print(\"… (total columnas Empresa:\", df_empresa.shape[1], \")\")\n",
        "print()\n",
        "\n",
        "print(\"=== NULOS POR COLUMNA — Base (TOP 10 con más nulos) ===\")\n",
        "nul_base = df_base.isna().sum().sort_values(ascending=False)                          # cuenta nulos por columna\n",
        "print(nul_base.head(10))                                                              # top 10 columnas más afectadas\n",
        "print(\"Nulos totales Base:\", int(nul_base.sum()))                                     # nulos totales\n",
        "print()\n",
        "\n",
        "print(\"=== NULOS POR COLUMNA — Con Empresa (TOP 10 con más nulos) ===\")\n",
        "nul_emp = df_empresa.isna().sum().sort_values(ascending=False)                        # idem para variante con Empresa\n",
        "print(nul_emp.head(10))\n",
        "print(\"Nulos totales Empresa:\", int(nul_emp.sum()))\n",
        "print()\n",
        "\n",
        "print(\"=== MUESTRAS (5 filas) — Base / Empresa / IDs ===\")\n",
        "display(df_base.head(5))                                                               # vistazo rápido a filas\n",
        "display(df_empresa.head(5))\n",
        "display(df_ids.head(5))\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 2) Identificación de grupos de columnas para el pipeline\n",
        "#    * No transformamos, solo mapeamos las listas que usaremos en Paso 2 *\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "# a) Presencias de atención (esperamos exactamente 4)\n",
        "PRES_COLS = [c for c in df_base.columns if c.endswith(\"_pres\")]                       # detecta columnas *_pres\n",
        "PRES_ESPERADAS = [\"Atencion15d_pres\",\"Atencion1m_pres\",\"Atencion3m_pres\",\"Atencion6m_pres\"]  # set acordado NB-02\n",
        "print(\"\\n=== PRESENCIAS DETECTADAS (Base) ===\")\n",
        "print(PRES_COLS)\n",
        "if set(PRES_COLS) != set(PRES_ESPERADAS):                                             # avisa si hay diferencias\n",
        "    print(\"⚠️ Ojo: diferencia con las esperadas:\", set(PRES_ESPERADAS) ^ set(PRES_COLS))\n",
        "\n",
        "# b) Numéricas clave (cantidades + montos log1p + KPIs + edad)\n",
        "NUMERIC_CORE = [\n",
        "    \"Edad\",\"CantPptos\",\"CantPptosAbo\",\"CantPptosAvan\",                                # cantidades y edad\n",
        "    \"TotPptos_l1p\",\"TotPptosAbo_l1p\",\"TotPptosAvan_l1p\",                              # montos log1p\n",
        "    \"PctCumplimiento\",\"TicketPromPpto_l1p\"                                            # KPIs\n",
        "]\n",
        "FALTAN_NUM = [c for c in NUMERIC_CORE if c not in df_base.columns]                    # verifica si falta alguna\n",
        "print(\"\\n=== NUMÉRICAS CLAVE (Base) ===\")\n",
        "print(NUMERIC_CORE)\n",
        "print(\"Faltantes:\", FALTAN_NUM if FALTAN_NUM else \"Ninguna\")\n",
        "\n",
        "# c) Dummies de geografía (Comuna y Región)\n",
        "DUM_COMUNA = [c for c in df_base.columns if c.startswith(\"Comuna_grp_\")]              # todas las dummies de comuna\n",
        "DUM_REGION = [c for c in df_base.columns if c.startswith(\"Region_\")]                  # todas las dummies de región\n",
        "print(\"\\n=== DUMMIES GEO — Comuna (n=\", len(DUM_COMUNA), \") ===\", sep=\"\")\n",
        "print(DUM_COMUNA[:15], (\"… (+)\" if len(DUM_COMUNA) > 15 else \"\"))                     # muestra primeras 15\n",
        "print(\"=== DUMMIES GEO — Región (n=\", len(DUM_REGION), \") ===\", sep=\"\")\n",
        "print(DUM_REGION)\n",
        "\n",
        "# d) Señal Empresa/Convenio (solo existen en df_empresa)\n",
        "EMP_BIN = [c for c in df_empresa.columns if c in (\"TieneEmpresa\",\"EsConvenio\")]       # flags binarios\n",
        "EMP_DUM = [c for c in df_empresa.columns if c.startswith(\"Empresa_grp_\")]             # dummies top-N por empresa\n",
        "print(\"\\n=== EMPRESA/CONVENIO — Binarios ===\")\n",
        "print(EMP_BIN if EMP_BIN else \"No detectados (esperado: en set con Empresa)\")\n",
        "print(\"=== EMPRESA/CONVENIO — Dummies (n=\", len(EMP_DUM), \") ===\", sep=\"\")\n",
        "print(EMP_DUM[:15], (\"… (+)\" if len(EMP_DUM) > 15 else \"\"))\n",
        "\n",
        "# e) Otras columnas potencialmente binarias (0/1) útiles\n",
        "#    *No son presencias ni geo/empresa, pero pueden ser señales tipo flag*\n",
        "OTRAS_BIN = []\n",
        "for c in df_base.columns:                                                             # iteramos todas las columnas\n",
        "    if c in NUMERIC_CORE or c in PRES_COLS:                                           # excluimos numéricas y presencias\n",
        "        continue\n",
        "    if c.startswith(\"Comuna_grp_\") or c.startswith(\"Region_\"):                        # excluimos geo\n",
        "        continue\n",
        "    if c.endswith(\"_grp\") or c.endswith(\"_cat\"):                                      # excluimos posibles labels no dummies\n",
        "        continue\n",
        "    serie = df_base[c]\n",
        "    # Detecta columnas con valores solo {0,1} (ignorando NaNs). Ojo con tipos object \"0\"/\"1\".\n",
        "    vals = pd.Series(serie.dropna().unique()).astype(str)\n",
        "    if set(vals).issubset({\"0\",\"1\"}):                                                 # si solo hay 0/1 como strings\n",
        "        OTRAS_BIN.append(c)\n",
        "\n",
        "print(\"\\n=== OTRAS BINARIAS 0/1 (Base) ===\")\n",
        "print(OTRAS_BIN if OTRAS_BIN else \"Ninguna detectada adicional\")\n",
        "\n",
        "# f) Columnas de identificación / metadata (para NO usarlas como features)\n",
        "ID_CANDIDATAS = []\n",
        "for c in df_ids.columns:                                                               # cualquier ID del archivo IDs\n",
        "    if c in df_base.columns:\n",
        "        ID_CANDIDATAS.append(c)\n",
        "print(\"\\n=== POSIBLES IDs PRESENTES EN Base (para excluir de features) ===\")\n",
        "print(ID_CANDIDATAS if ID_CANDIDATAS else \"No hay IDs del archivo IDs dentro de Base (ideal)\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 3) Construcción de LISTAS FINALES de columnas por variante\n",
        "#    * Estas listas se usarán en el Paso 2 para armar el ColumnTransformer *\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "# LISTAS PARA VARIANTE BASELINE (SIN EMPRESA)\n",
        "NUM_COLS_BASELINE   = [c for c in NUMERIC_CORE if c in df_base.columns]               # numéricas presentes\n",
        "PRES_COLS_BASELINE  = PRES_COLS[:]                                                    # presencias detectadas\n",
        "GEO_DUM_BASELINE    = DUM_COMUNA + DUM_REGION                                         # todas las geo dummies\n",
        "BIN_COLS_BASELINE   = OTRAS_BIN[:]                                                    # otras binarias 0/1 (si las hubiera)\n",
        "\n",
        "# LISTAS PARA VARIANTE CON EMPRESA\n",
        "NUM_COLS_EMPRESA    = [c for c in NUMERIC_CORE if c in df_empresa.columns]            # numéricas presentes\n",
        "PRES_COLS_EMPRESA   = [c for c in PRES_COLS if c in df_empresa.columns]               # presencias en el set con empresa\n",
        "GEO_DUM_EMPRESA     = [c for c in DUM_COMUNA + DUM_REGION if c in df_empresa.columns] # geo dummies presentes\n",
        "EMP_BIN_COLS        = [c for c in EMP_BIN if c in df_empresa.columns]                 # TieneEmpresa / EsConvenio\n",
        "EMP_DUM_COLS        = EMP_DUM[:]                                                      # Empresa_grp_* detectadas\n",
        "BIN_COLS_EMPRESA    = [c for c in OTRAS_BIN if c in df_empresa.columns]               # otras binarias 0/1 también en este set\n",
        "\n",
        "# Muestra un resumen compacto de las listas (para revisar a ojo)\n",
        "def _resumen_listas(nombre, lst, max_show=12):\n",
        "    vista = lst[:max_show] + ([\"… (+)\"] if len(lst) > max_show else [])\n",
        "    print(f\"{nombre:24s} (n={len(lst):>3}):\", vista)\n",
        "\n",
        "print(\"\\n=== LISTAS DE FEATURES — BASELINE (sin Empresa) ===\")\n",
        "_resumen_listas(\"NUM_COLS_BASELINE\",  NUM_COLS_BASELINE)\n",
        "_resumen_listas(\"PRES_COLS_BASELINE\", PRES_COLS_BASELINE)\n",
        "_resumen_listas(\"GEO_DUM_BASELINE\",   GEO_DUM_BASELINE)\n",
        "_resumen_listas(\"BIN_COLS_BASELINE\",  BIN_COLS_BASELINE)\n",
        "\n",
        "print(\"\\n=== LISTAS DE FEATURES — CON EMPRESA ===\")\n",
        "_resumen_listas(\"NUM_COLS_EMPRESA\",   NUM_COLS_EMPRESA)\n",
        "_resumen_listas(\"PRES_COLS_EMPRESA\",  PRES_COLS_EMPRESA)\n",
        "_resumen_listas(\"GEO_DUM_EMPRESA\",    GEO_DUM_EMPRESA)\n",
        "_resumen_listas(\"EMP_BIN_COLS\",       EMP_BIN_COLS)\n",
        "_resumen_listas(\"EMP_DUM_COLS\",       EMP_DUM_COLS)\n",
        "_resumen_listas(\"BIN_COLS_EMPRESA\",   BIN_COLS_EMPRESA)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 4) Mini-checks ligeros (no son \"tests duros\", solo semáforo)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "print(\"\\n=== MINI-CHECKS DE AUDITORÍA ===\")\n",
        "print(\"Presencias = 4 ?                    \", \"OK ✅\" if set(PRES_COLS)==set(PRES_ESPERADAS) else \"Revisar ⚠️\")\n",
        "print(\"Numéricas clave faltantes en Base ? \", \"Ninguna ✅\" if len(FALTAN_NUM)==0 else f\"Faltan: {FALTAN_NUM} ⚠️\")\n",
        "print(\"¿Hay dummies de Comuna?             \", \"Sí ✅\" if len(DUM_COMUNA)>0 else \"No ⚠️\")\n",
        "print(\"¿Hay dummies de Región?             \", \"Sí ✅\" if len(DUM_REGION)>0 else \"No ⚠️\")\n",
        "print(\"¿Se detectan EMP_BIN en Empresa?    \", \"Sí ✅\" if len(EMP_BIN)>0 else \"No ⚠️ (esperado si no aplica)\")\n",
        "print(\"¿Se detectan EMP_DUM en Empresa?    \", \"Sí ✅\" if len(EMP_DUM)>0 else \"No ⚠️ (según corte)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb912c41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PASO 2 — ESTANDARIZACIÓN Y PIPELINES (NB-03)\n",
        "# ============================================================\n",
        "# Objetivo: preparar los datasets para clustering.\n",
        "# Creamos un ColumnTransformer que estandariza numéricas y deja\n",
        "# tal cual (passthrough) presencias, dummies y flags binarios.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler   # escalador por defecto\n",
        "from sklearn.compose import ColumnTransformer      # permite aplicar transformaciones distintas por columna\n",
        "from sklearn.pipeline import Pipeline              # armar pipeline de pasos consecutivos\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 1) Definimos transformador para NUMÉRICAS\n",
        "# ────────────────────────────────────────────────────────────\n",
        "scaler = StandardScaler()   # escalará cada numérica a media=0 y std=1\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 2) Pipeline para VARIANTE BASELINE (sin Empresa)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "ct_base = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", scaler, NUM_COLS_BASELINE),        # aplica StandardScaler a las numéricas\n",
        "        (\"pres\", \"passthrough\", PRES_COLS_BASELINE),  # deja pasar las presencias (0/1)\n",
        "        (\"geo\", \"passthrough\", GEO_DUM_BASELINE),     # deja pasar dummies geográficas\n",
        "        (\"bin\", \"passthrough\", BIN_COLS_BASELINE)     # deja pasar otras binarias 0/1\n",
        "    ],\n",
        "    remainder=\"drop\"  # cualquier columna no listada se descarta\n",
        ")\n",
        "\n",
        "pipeline_base = Pipeline([\n",
        "    (\"transform\", ct_base)   # paso único por ahora (solo transformar)\n",
        "])\n",
        "\n",
        "# Ajustamos (fit) y transformamos (transform) el dataset baseline\n",
        "X_base_ready = pipeline_base.fit_transform(df_base)\n",
        "\n",
        "print(\"=== VARIANTE BASELINE (sin Empresa) ===\")\n",
        "print(\"Shape original:\", df_base.shape)                  # muestra shape original\n",
        "print(\"Shape transformado:\", X_base_ready.shape)         # filas deben coincidir; cols = suma de todas las listas\n",
        "print(\"NaNs en transformado:\", np.isnan(X_base_ready).sum())  # NaNs deberían ser 0\n",
        "print()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 3) Pipeline para VARIANTE CON EMPRESA\n",
        "# ────────────────────────────────────────────────────────────\n",
        "ct_emp = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", scaler, NUM_COLS_EMPRESA),           # StandardScaler a numéricas\n",
        "        (\"pres\", \"passthrough\", PRES_COLS_EMPRESA),  # presencias\n",
        "        (\"geo\", \"passthrough\", GEO_DUM_EMPRESA),     # dummies geo\n",
        "        (\"emp_bin\", \"passthrough\", EMP_BIN_COLS),    # TieneEmpresa, EsConvenio\n",
        "        (\"emp_dum\", \"passthrough\", EMP_DUM_COLS),    # Empresa_grp_*\n",
        "        (\"bin\", \"passthrough\", BIN_COLS_EMPRESA)     # otras binarias 0/1\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "pipeline_emp = Pipeline([\n",
        "    (\"transform\", ct_emp)\n",
        "])\n",
        "\n",
        "X_emp_ready = pipeline_emp.fit_transform(df_empresa)\n",
        "\n",
        "print(\"=== VARIANTE CON EMPRESA ===\")\n",
        "print(\"Shape original:\", df_empresa.shape)\n",
        "print(\"Shape transformado:\", X_emp_ready.shape)\n",
        "print(\"NaNs en transformado:\", np.isnan(X_emp_ready).sum())\n",
        "print()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 4) Mini-checks ligeros\n",
        "# ────────────────────────────────────────────────────────────\n",
        "print(\"=== MINI-CHECKS ===\")\n",
        "print(\"Baseline: filas coinciden?\", X_base_ready.shape[0] == df_base.shape[0])\n",
        "print(\"Empresa: filas coinciden?\", X_emp_ready.shape[0] == df_empresa.shape[0])\n",
        "print(\"¿Baseline sin NaNs?\", np.isnan(X_base_ready).sum() == 0)\n",
        "print(\"¿Con Empresa sin NaNs?\", np.isnan(X_emp_ready).sum() == 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904594f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PASO 2b — REVISIÓN DE DISTRIBUCIÓN (numéricas clave)\n",
        "# ============================================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 1) Seleccionar las columnas numéricas clave del baseline\n",
        "# ────────────────────────────────────────────────────────────\n",
        "df_num = df_base[NUM_COLS_BASELINE].copy()    # subset solo con numéricas\n",
        "print(\"Shape numéricas (baseline):\", df_num.shape)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 2) Estadísticos descriptivos extendidos\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Usamos .describe() para percentiles básicos, y agregamos p90/p99\n",
        "desc = df_num.describe(percentiles=[0.9, 0.99]).T   # .T = transpuesta, filas=variables\n",
        "print(\"=== ESTADÍSTICOS DE NUMÉRICAS (Baseline) ===\")\n",
        "print(desc[[\"mean\",\"50%\",\"90%\",\"99%\",\"min\",\"max\"]])  # mostramos solo columnas clave\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 3) Histogramas para cada columna numérica\n",
        "# ────────────────────────────────────────────────────────────\n",
        "for col in NUM_COLS_BASELINE:\n",
        "    plt.figure(figsize=(6,4))                           # tamaño del gráfico\n",
        "    plt.hist(df_num[col], bins=50, edgecolor=\"black\")   # histograma con 50 bins\n",
        "    plt.title(f\"Distribución de {col}\")                 # título con nombre de variable\n",
        "    plt.xlabel(col)                                     # eje x\n",
        "    plt.ylabel(\"Frecuencia\")                            # eje y\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)           # rejilla ligera\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb66d4d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PASO 2c — PIPELINE MIXTO (Standard + Robust + Passthrough)\n",
        "# ============================================================\n",
        "# Objetivo: escalar cada grupo de variables con la técnica adecuada\n",
        "# y dejar listas las matrices X para clustering (sin entrenar aún).\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler   # dos escaladores\n",
        "from sklearn.compose import ColumnTransformer                    # mezcla de transformaciones por columna\n",
        "from sklearn.pipeline import Pipeline                            # pipeline para encadenar pasos\n",
        "import numpy as np                                               # chequeos de NaN\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 1) Definimos qué columnas van con cada tratamiento\n",
        "#    (partimos de NUM_COLS_* que creaste en el Paso 1)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "# a) StandardScaler → variables aproximadamente “normales”\n",
        "STD_COLS_CANON = [\"Edad\"]                                        # en tu análisis, Edad se comporta razonable\n",
        "\n",
        "# b) RobustScaler → variables con colas largas / muchos ceros\n",
        "ROB_COLS_CANON = [\n",
        "    \"CantPptos\", \"CantPptosAbo\", \"CantPptosAvan\",                # conteos sesgados\n",
        "    \"TotPptos_l1p\", \"TotPptosAbo_l1p\", \"TotPptosAvan_l1p\",       # montos (log1p) con masa en 0 y colas\n",
        "    \"TicketPromPpto_l1p\"\n",
        "]\n",
        "\n",
        "# c) Passthrough → ya normalizada (0–1), no necesita escalar\n",
        "PASS_COLS_CANON = [\"PctCumplimiento\"]\n",
        "\n",
        "# Nota: Por seguridad, intersectamos con las columnas PRESENTES en cada dataset\n",
        "def _split_numeric_lists(cols_present):\n",
        "    \"\"\"Devuelve tres listas (std, rob, pass) válidas para el dataset dado.\"\"\"\n",
        "    std_cols  = [c for c in STD_COLS_CANON  if c in cols_present]\n",
        "    rob_cols  = [c for c in ROB_COLS_CANON  if c in cols_present]\n",
        "    pass_cols = [c for c in PASS_COLS_CANON if c in cols_present]\n",
        "    # Si por alguna razón falta una numérica esperada, no crashea; simplemente no la usa.\n",
        "    return std_cols, rob_cols, pass_cols\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 2) Construimos el ColumnTransformer para CADA VARIANTE\n",
        "#    (baseline sin Empresa y con Empresa)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "# 2.1) VARIANTE BASELINE (sin Empresa)\n",
        "std_cols_base, rob_cols_base, pass_cols_base = _split_numeric_lists(NUM_COLS_BASELINE)\n",
        "\n",
        "ct_base_mixto = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num_std\", StandardScaler(), std_cols_base),            # escala Edad (u otras que asignes)\n",
        "        (\"num_rob\", RobustScaler(),   rob_cols_base),            # escala montos y conteos sesgados\n",
        "        (\"num_pas\", \"passthrough\",    pass_cols_base),           # deja PctCumplimiento tal cual\n",
        "        (\"pres\",    \"passthrough\",    PRES_COLS_BASELINE),       # presencias 0/1\n",
        "        (\"geo\",     \"passthrough\",    GEO_DUM_BASELINE),         # dummies geográficas\n",
        "        (\"bin\",     \"passthrough\",    BIN_COLS_BASELINE),        # otras binarias 0/1\n",
        "    ],\n",
        "    remainder=\"drop\"                                             # descarta cualquier columna no listada\n",
        ")\n",
        "\n",
        "pipeline_base_mixto = Pipeline([\n",
        "    (\"transform\", ct_base_mixto)                                 # por ahora, solo transformar\n",
        "])\n",
        "\n",
        "X_base_ready = pipeline_base_mixto.fit_transform(df_base)        # fit+transform para baseline\n",
        "\n",
        "print(\"=== VARIANTE BASELINE (mixto) ===\")\n",
        "print(\"Original (filas, cols):\", df_base.shape)\n",
        "print(\"Transformado (filas, cols):\", X_base_ready.shape)\n",
        "print(\"NaNs en transformado:\", int(np.isnan(X_base_ready).sum()))\n",
        "print()\n",
        "\n",
        "# 2.2) VARIANTE CON EMPRESA\n",
        "std_cols_emp, rob_cols_emp, pass_cols_emp = _split_numeric_lists(NUM_COLS_EMPRESA)\n",
        "\n",
        "ct_emp_mixto = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num_std\",  StandardScaler(), std_cols_emp),            # escala Edad (si está)\n",
        "        (\"num_rob\",  RobustScaler(),   rob_cols_emp),            # escala montos y conteos sesgados\n",
        "        (\"num_pas\",  \"passthrough\",    pass_cols_emp),           # PctCumplimiento\n",
        "        (\"pres\",     \"passthrough\",    PRES_COLS_EMPRESA),       # presencias 0/1\n",
        "        (\"geo\",      \"passthrough\",    GEO_DUM_EMPRESA),         # dummies geográficas\n",
        "        (\"emp_bin\",  \"passthrough\",    EMP_BIN_COLS),            # TieneEmpresa / EsConvenio\n",
        "        (\"emp_dum\",  \"passthrough\",    EMP_DUM_COLS),            # Empresa_grp_*\n",
        "        (\"bin\",      \"passthrough\",    BIN_COLS_EMPRESA),        # otras binarias 0/1\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "pipeline_emp_mixto = Pipeline([\n",
        "    (\"transform\", ct_emp_mixto)\n",
        "])\n",
        "\n",
        "X_emp_ready = pipeline_emp_mixto.fit_transform(df_empresa)       # fit+transform para set con Empresa\n",
        "\n",
        "print(\"=== VARIANTE CON EMPRESA (mixto) ===\")\n",
        "print(\"Original (filas, cols):\", df_empresa.shape)\n",
        "print(\"Transformado (filas, cols):\", X_emp_ready.shape)\n",
        "print(\"NaNs en transformado:\", int(np.isnan(X_emp_ready).sum()))\n",
        "print()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 3) Mini-checks rápidos para tu tranquilidad\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def _ok(flag): return \"OK ✅\" if flag else \"Revisar ⚠️\"\n",
        "\n",
        "print(\"=== MINI-CHECKS ===\")\n",
        "print(\"Baseline: filas conservadas      :\", _ok(X_base_ready.shape[0] == df_base.shape[0]))\n",
        "print(\"Baseline: sin NaNs               :\", _ok(np.isnan(X_base_ready).sum() == 0))\n",
        "print(\"Con Empresa: filas conservadas   :\", _ok(X_emp_ready.shape[0] == df_empresa.shape[0]))\n",
        "print(\"Con Empresa: sin NaNs            :\", _ok(np.isnan(X_emp_ready).sum() == 0))\n",
        "\n",
        "# (Opcional) Si quieres guardar las columnas usadas por transformador:\n",
        "print(\"\\nListas usadas (baseline):\")\n",
        "print(\"  STD :\", std_cols_base)\n",
        "print(\"  ROB :\", rob_cols_base)\n",
        "print(\"  PASS:\", pass_cols_base)\n",
        "print(\"Listas usadas (con empresa):\")\n",
        "print(\"  STD :\", std_cols_emp)\n",
        "print(\"  ROB :\", rob_cols_emp)\n",
        "print(\"  PASS:\", pass_cols_emp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e99cee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PASO 3 — BARRIDO DE K (2..10) con K-Means — 100% comentado\n",
        "# ============================================================\n",
        "\n",
        "# 1) Imports necesarios para clustering, métricas, gráficos y tabla\n",
        "from sklearn.cluster import KMeans              # algoritmo K-Means para agrupar\n",
        "from sklearn.metrics import silhouette_score   # métrica para evaluar separación entre clusters\n",
        "import matplotlib.pyplot as plt                # gráficos (usamos matplotlib, no seaborn)\n",
        "import numpy as np                             # utilidades numéricas\n",
        "import pandas as pd                            # para armar tabla resumen\n",
        "\n",
        "# 2) Validaciones suaves: asegurarnos de que las matrices transformadas existen\n",
        "assert 'X_base_ready' in globals(), \"X_base_ready no existe. Ejecuta el Paso 2c (pipeline mixto) primero.\"\n",
        "assert 'X_emp_ready'  in globals(), \"X_emp_ready no existe. Ejecuta el Paso 2c (pipeline mixto) primero.\"\n",
        "\n",
        "# 3) Definimos una función que recorre K entre k_min y k_max y guarda las métricas\n",
        "def barrido_k(X, k_min=2, k_max=10, random_state=42):        # recibe matriz X ya transformada y rango de K\n",
        "    inertias = []                                            # lista vacía para la inercia (SSE) por cada K\n",
        "    silhouettes = []                                         # lista vacía para el silhouette por cada K\n",
        "    for k in range(k_min, k_max + 1):                        # iteramos K = 2,3,...,k_max\n",
        "        km = KMeans(                                         # instanciamos el modelo K-Means\n",
        "            n_clusters=k,                                    # número de clusters a probar\n",
        "            random_state=random_state,                       # semilla para reproducibilidad\n",
        "            n_init=10                                        # nº de reinicios para evitar malos mínimos locales\n",
        "        )\n",
        "        labels = km.fit_predict(X)                           # ajusta el modelo y devuelve etiquetas para cada fila\n",
        "        inertias.append(km.inertia_)                         # guardamos la inercia (SSE dentro de clusters)\n",
        "        sil = silhouette_score(X, labels) if k > 1 else np.nan  # silhouette requiere al menos 2 clusters\n",
        "        silhouettes.append(sil)                              # guardamos el silhouette de este K\n",
        "    return inertias, silhouettes                             # devolvemos ambas listas\n",
        "\n",
        "# 4) Ejecutamos el barrido para la variante BASELINE (sin Empresa)\n",
        "inertias_base, sils_base = barrido_k(                        # llamamos la función de barrido\n",
        "    X=X_base_ready,                                          # matriz X ya escalada/transformada (baseline)\n",
        "    k_min=2,                                                 # K mínimo a evaluar\n",
        "    k_max=10,                                                # K máximo a evaluar\n",
        "    random_state=42                                          # semilla fija\n",
        ")\n",
        "\n",
        "# 5) Ejecutamos el barrido para la variante CON EMPRESA\n",
        "inertias_emp, sils_emp = barrido_k(                          # repetimos el barrido para la otra variante\n",
        "    X=X_emp_ready,                                           # matriz X con Empresa/Convenio\n",
        "    k_min=2,                                                 # mismo rango de K\n",
        "    k_max=10,                                                # mismo rango de K\n",
        "    random_state=42                                          # misma semilla\n",
        ")\n",
        "\n",
        "# 6) Construimos la lista de valores K evaluados (2..10)\n",
        "K_range = list(range(2, 11))                                 # lista [2,3,...,10] para alinear con resultados\n",
        "\n",
        "# 7) Graficamos la CURVA DEL CODO (Inertia) para comparar ambas variantes\n",
        "plt.figure(figsize=(12, 5))                                  # creamos una figura de 12x5 pulgadas\n",
        "plt.subplot(1, 2, 1)                                         # primer panel (izquierda) de 1 fila x 2 columnas\n",
        "plt.plot(K_range, inertias_base, \"o-\", label=\"Baseline\")     # trazamos inercia de baseline (marcador círculo)\n",
        "plt.plot(K_range, inertias_emp, \"s--\", label=\"Con Empresa\")  # trazamos inercia con empresa (marcador cuadrado)\n",
        "plt.xlabel(\"Número de clusters (K)\")                         # etiqueta eje X\n",
        "plt.ylabel(\"Inertia (SSE)\")                                  # etiqueta eje Y\n",
        "plt.title(\"Curva del codo (SSE)\")                            # título del gráfico\n",
        "plt.legend()                                                 # mostramos leyenda para distinguir variantes\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)                    # rejilla suave para leer mejor\n",
        "\n",
        "# 8) Graficamos el SILHOUETTE para comparar ambas variantes\n",
        "plt.subplot(1, 2, 2)                                         # segundo panel (derecha)\n",
        "plt.plot(K_range, sils_base, \"o-\", label=\"Baseline\")         # silhouette baseline\n",
        "plt.plot(K_range, sils_emp, \"s--\", label=\"Con Empresa\")      # silhouette con empresa\n",
        "plt.xlabel(\"Número de clusters (K)\")                         # etiqueta eje X\n",
        "plt.ylabel(\"Silhouette score\")                               # etiqueta eje Y\n",
        "plt.title(\"Comparación de silhouette\")                       # título del gráfico\n",
        "plt.legend()                                                 # leyenda para distinguir líneas\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)                    # rejilla suave\n",
        "\n",
        "plt.tight_layout()                                           # ajusta espacios entre subplots automáticamente\n",
        "plt.show()                                                   # muestra los gráficos\n",
        "\n",
        "# 9) Armamos una TABLA RESUMEN con todos los valores para inspección fina\n",
        "tabla_resumen = pd.DataFrame({                               # construimos un DataFrame con columnas:\n",
        "    \"K\": K_range,                                            # K evaluado\n",
        "    \"Inertia_Base\": inertias_base,                           # inercia para baseline\n",
        "    \"Silhouette_Base\": sils_base,                            # silhouette para baseline\n",
        "    \"Inertia_Emp\": inertias_emp,                             # inercia para con empresa\n",
        "    \"Silhouette_Emp\": sils_emp                               # silhouette para con empresa\n",
        "})\n",
        "\n",
        "# 10) Calculamos sugerencias automáticas (no decide, solo guía)\n",
        "k_mejor_sil_base = int(tabla_resumen.loc[tabla_resumen[\"Silhouette_Base\"].idxmax(), \"K\"])  # K con mayor silhouette (baseline)\n",
        "k_mejor_sil_emp  = int(tabla_resumen.loc[tabla_resumen[\"Silhouette_Emp\"].idxmax(), \"K\"])   # K con mayor silhouette (con empresa)\n",
        "\n",
        "# 11) Mostramos la tabla y las sugerencias por silhouette\n",
        "print(\"=== TABLA RESUMEN — BARRIDO K ===\")                  # cabecera de la tabla\n",
        "display(tabla_resumen)                                     # mostramos la tabla en el notebook\n",
        "print(f\"Sugerencia por silhouette (Baseline): K = {k_mejor_sil_base}\")  # sugerencia baseline\n",
        "print(f\"Sugerencia por silhouette (Con Empresa): K = {k_mejor_sil_emp}\")# sugerencia con empresa\n",
        "\n",
        "# 12) (Opcional) Heurística simple de \"codo\": diferencia relativa de SSE\n",
        "#     Nota: esto NO es una elección automática. Solo te indica dónde el SSE deja de bajar fuerte.\n",
        "sse_base = np.array(inertias_base)                          # convertimos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f09053b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PASO 4 — ENTRENAMIENTO FINAL K=3\n",
        "# ============================================================\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 1) Definimos función auxiliar para entrenar y devolver etiquetas + modelo\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def entrenar_kmeans(X, n_clusters=3, random_state=42):\n",
        "    \"\"\"\n",
        "    Entrena KMeans con K clusters y devuelve:\n",
        "    - modelo entrenado\n",
        "    - etiquetas de cluster para cada fila\n",
        "    \"\"\"\n",
        "    km = KMeans(\n",
        "        n_clusters=n_clusters,       # número de clusters\n",
        "        random_state=random_state,   # semilla reproducible\n",
        "        n_init=10                    # nº de reinicios (reduce riesgo de mínimos locales)\n",
        "    )\n",
        "    labels = km.fit_predict(X)       # entrena y asigna cluster a cada fila\n",
        "    return km, labels\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 2) Entrenamos con VARIANTE BASELINE (sin Empresa)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "km_base, labels_base = entrenar_kmeans(X_base_ready, n_clusters=3)\n",
        "\n",
        "# Añadimos etiquetas al DataFrame de IDs (para vincular con paciente)\n",
        "df_ids[\"cluster_baseline\"] = labels_base\n",
        "\n",
        "print(\"=== VARIANTE BASELINE ===\")\n",
        "print(\"Tamaño por cluster:\")\n",
        "print(pd.Series(labels_base).value_counts().sort_index())   # conteo de pacientes por cluster\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 3) Entrenamos con VARIANTE CON EMPRESA\n",
        "# ────────────────────────────────────────────────────────────\n",
        "km_emp, labels_emp = entrenar_kmeans(X_emp_ready, n_clusters=3)\n",
        "\n",
        "# Añadimos etiquetas también\n",
        "df_ids[\"cluster_con_empresa\"] = labels_emp\n",
        "\n",
        "print(\"\\n=== VARIANTE CON EMPRESA ===\")\n",
        "print(\"Tamaño por cluster:\")\n",
        "print(pd.Series(labels_emp).value_counts().sort_index())\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 4) Guardamos centroides (en espacio transformado)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "centroides_base = pd.DataFrame(\n",
        "    km_base.cluster_centers_,              # centroides (matriz K x nº_features)\n",
        "    columns=ct_base_mixto.get_feature_names_out(),  # nombres de features después del transformer\n",
        ")\n",
        "centroides_base[\"cluster\"] = centroides_base.index\n",
        "\n",
        "centroides_emp = pd.DataFrame(\n",
        "    km_emp.cluster_centers_,\n",
        "    columns=ct_emp_mixto.get_feature_names_out(),\n",
        ")\n",
        "centroides_emp[\"cluster\"] = centroides_emp.index\n",
        "\n",
        "print(\"\\n=== Centroides (Baseline, primeras columnas) ===\")\n",
        "display(centroides_base.iloc[:,:10])   # mostramos solo 10 primeras cols para no saturar\n",
        "\n",
        "print(\"\\n=== Centroides (Con Empresa, primeras columnas) ===\")\n",
        "display(centroides_emp.iloc[:,:10])\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 5) Perfiles en espacio original (medianas por cluster)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Baseline\n",
        "perfil_base = df_base.copy()\n",
        "perfil_base[\"cluster_baseline\"] = labels_base\n",
        "perfil_base = perfil_base.groupby(\"cluster_baseline\").median(numeric_only=True)\n",
        "\n",
        "# Con Empresa\n",
        "perfil_emp = df_empresa.copy()\n",
        "perfil_emp[\"cluster_con_empresa\"] = labels_emp\n",
        "perfil_emp = perfil_emp.groupby(\"cluster_con_empresa\").median(numeric_only=True)\n",
        "\n",
        "print(\"\\n=== Perfiles por mediana (Baseline) ===\")\n",
        "display(perfil_base[NUM_COLS_BASELINE + PRES_COLS_BASELINE].round(2))  # mostramos numéricas+presencias\n",
        "\n",
        "print(\"\\n=== Perfiles por mediana (Con Empresa) ===\")\n",
        "display(perfil_emp[NUM_COLS_EMPRESA[:10]].round(2))  # mostramos primeras 10 numéricas para ver patrón\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 6) Export opcional de resultados (para NB-04 y validación)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# IDs + clusters\n",
        "df_ids.to_csv(ROOT / \"data\" / \"processed\" / \"activos_ids_v2_plus_clustered.csv\", index=False)\n",
        "\n",
        "# Centroides\n",
        "centroides_base.to_csv(ROOT / \"data\" / \"processed\" / \"clusters_centroids_baseline.csv\", index=False)\n",
        "centroides_emp.to_csv(ROOT / \"data\" / \"processed\" / \"clusters_centroids_con_empresa.csv\", index=False)\n",
        "\n",
        "# Perfiles\n",
        "perfil_base.to_csv(ROOT / \"data\" / \"processed\" / \"cluster_profiles_baseline.csv\")\n",
        "perfil_emp.to_csv(ROOT / \"data\" / \"processed\" / \"cluster_profiles_con_empresa.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583c4400",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PASO 5 — PERFILADO E INSIGHTS ACCIONABLES\n",
        "# ============================================================\n",
        "# Objetivo: traducir los perfiles de clusters en un resumen humano,\n",
        "# con insights útiles para la gestión (retención, reactivación, depuración).\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 1) Preparamos un diccionario con \"narrativas\" de cada cluster\n",
        "# ────────────────────────────────────────────────────────────\n",
        "insights_baseline = {\n",
        "    0: {\n",
        "        \"nombre\": \"Uso bajo / abandono latente\",\n",
        "        \"perfil\": \"Pacientes de ~35 años, con 1 presupuesto de monto medio, cumplimiento parcial (≈67%), sin atenciones en últimos meses.\",\n",
        "        \"acciones\": [\n",
        "            \"Campañas de reactivación (recordatorios, WhatsApp, email).\",\n",
        "            \"Ofertas de control preventivo o chequeos básicos.\",\n",
        "            \"Identificar causas de abandono (precio, experiencia, distancia).\"\n",
        "        ]\n",
        "    },\n",
        "    1: {\n",
        "        \"nombre\": \"Alta frecuencia / alta conversión\",\n",
        "        \"perfil\": \"Pacientes jóvenes (~26 años), con ~5 presupuestos, alta tasa de cumplimiento (≈80%), con atenciones en últimos 6 meses.\",\n",
        "        \"acciones\": [\n",
        "            \"Enfocar en retención y fidelización (programas de membresía, descuentos por continuidad).\",\n",
        "            \"Promover recomendaciones y derivaciones (traer amigos/familia).\",\n",
        "            \"Monitorear satisfacción para evitar pérdida de este segmento clave.\"\n",
        "        ]\n",
        "    },\n",
        "    2: {\n",
        "        \"nombre\": \"Inactivos / fantasma\",\n",
        "        \"perfil\": \"Pacientes de ~38 años, sin presupuestos activos, sin pagos ni atenciones recientes.\",\n",
        "        \"acciones\": [\n",
        "            \"Revisar calidad del registro (¿pacientes duplicados, históricos?).\",\n",
        "            \"Campañas muy ligeras (email masivo o SMS) → bajo costo.\",\n",
        "            \"Probable depuración de base si no responden.\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 2) Imprimimos en formato legible (como brief para gerente)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "print(\"=== INSIGHTS CLUSTERS (Baseline) ===\\n\")\n",
        "for cluster, info in insights_baseline.items():\n",
        "    print(f\"Cluster {cluster} — {info['nombre']}\")\n",
        "    print(f\"Perfil: {info['perfil']}\")\n",
        "    print(\"Acciones sugeridas:\")\n",
        "    for act in info[\"acciones\"]:\n",
        "        print(f\"  - {act}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 3) (Opcional) Export a un .md (Markdown) para usar directo en Notion/GitHub\n",
        "# ────────────────────────────────────────────────────────────\n",
        "output_md = ROOT / \"reports\" / \"cluster_insights_baseline.md\"\n",
        "output_md.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(output_md, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"# Insights Clusters (Baseline)\\n\\n\")\n",
        "    for cluster, info in insights_baseline.items():\n",
        "        f.write(f\"## Cluster {cluster} — {info['nombre']}\\n\")\n",
        "        f.write(f\"**Perfil:** {info['perfil']}\\n\\n\")\n",
        "        f.write(\"**Acciones sugeridas:**\\n\")\n",
        "        for act in info[\"acciones\"]:\n",
        "            f.write(f\"- {act}\\n\")\n",
        "        f.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0616ad89",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PASO 5b — COMPARATIVA BASELINE vs CON EMPRESA\n",
        "# ============================================================\n",
        "# Objetivo: mostrar si incluir Empresa/Convenio aporta algo nuevo al clustering.\n",
        "# Vamos a comparar:\n",
        "#   1. Tamaños de clusters.\n",
        "#   2. Perfiles (medianas).\n",
        "#   3. Conclusión narrativa.\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 1) Conteo de pacientes por cluster en cada variante\n",
        "# ────────────────────────────────────────────────────────────\n",
        "conteos_base = pd.Series(labels_base).value_counts().sort_index()\n",
        "conteos_emp  = pd.Series(labels_emp).value_counts().sort_index()\n",
        "\n",
        "print(\"=== Conteo de pacientes por cluster ===\")\n",
        "print(\"Baseline (sin Empresa):\")\n",
        "print(conteos_base)\n",
        "print(\"\\nCon Empresa:\")\n",
        "print(conteos_emp)\n",
        "print()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 2) Diferencia absoluta de conteos\n",
        "# ────────────────────────────────────────────────────────────\n",
        "diff_conteos = conteos_emp - conteos_base\n",
        "print(\"Diferencia en nº de pacientes (Con Empresa - Baseline):\")\n",
        "print(diff_conteos)\n",
        "print()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 3) Comparar perfiles de mediana en variables clave\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Seleccionamos las columnas numéricas comunes\n",
        "cols_clave = [\"Edad\",\"CantPptos\",\"TotPptos_l1p\",\"PctCumplimiento\",\"TicketPromPpto_l1p\"]\n",
        "\n",
        "comparativa_perfiles = pd.concat([\n",
        "    perfil_base[cols_clave].add_suffix(\"_Base\"),\n",
        "    perfil_emp[cols_clave].add_suffix(\"_Emp\")\n",
        "], axis=1)\n",
        "\n",
        "print(\"=== Comparativa de perfiles (medianas) ===\")\n",
        "display(comparativa_perfiles.round(2))\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# 4) Conclusión narrativa\n",
        "# ────────────────────────────────────────────────────────────\n",
        "print(\"=== Conclusión narrativa ===\")\n",
        "print(\"La distribución de pacientes por cluster es prácticamente idéntica en ambas variantes.\")\n",
        "print(\"Los perfiles (edad, nº de presupuestos, ticket promedio, % cumplimiento) son casi iguales.\")\n",
        "print(\"→ Esto indica que las variables de Empresa/Convenio NO aportan diferenciación adicional significativa.\")\n",
        "print(\"→ La dinámica de uso clínico y presupuestos domina el agrupamiento.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2163df",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
